model_name: Allegro-MP-L # required (this must match the model's label which is the 3rd arg in the matbench_discovery.preds.Model enum)
model_key: allegro-MP-L-0.1 # this should match the name of the YAML file and determines the URL /models/<model_key> on which details of the model are displayed on the website
model_version: '0.1'
date_added: '2025-08-28'
date_published: '2025-08-28'
authors:
  - name: Seán R. Kavanagh
    affiliation: Center for the Environment, Harvard University & MIR Group, Harvard University
    email: skavanagh@seas.harvard.edu
    orcid: https://orcid.org/0000-0003-4577-9647
    url: https://seankavanagh.com
    corresponding: true
  - name: Chuin Wei Tan
    affiliation: MIR Group, Harvard University
  - name: Albert Musaelian
    affiliation: MIR Group, Harvard University & Mirian Technologies
  - name: William C. Witt
    affiliation: MIR Group, Harvard University
  - name: Gabriel de Miranda Nascimento
    affiliation: MIR Group, Harvard University & MIT
  - name: Ulrik Unneberg
    affiliation: MIR Group, Harvard University & MIT
  - name: Marc L. Descoteaux
    affiliation: MIR Group, Harvard University
  - name: Boris Kozinsky
    affiliation: MIR Group, Harvard University
trained_by:
  - name: Seán R. Kavanagh
    affiliation: Center for the Environment, Harvard University & MIR Group, Harvard University
    email: skavanagh@seas.harvard.edu
    orcid: https://orcid.org/0000-0003-4577-9647
    url: https://seankavanagh.com
    corresponding: true

repo: https://github.com/mir-group/allegro
url: https://allegro.readthedocs.io/en/latest/
doi: https://doi.org/10.5281/zenodo.16980200
paper: https://arxiv.org/abs/2504.16068 # To be updated...
checkpoint_url: https://www.allegro.net/models/mir-group/Allegro-MP-L:0.1
pr_url: https://github.com/janosh/matbench-discovery/pull/282

openness: OSOD # see `Open` enum in matbench_discovery/enums.py
train_task: S2EFS # see `Task` enum in matbench_discovery/enums.py
test_task: IS2RE-SR # see `Task` enum in matbench_discovery/enums.py
targets: EFS_G # see `Targets` enum in matbench_discovery/enums.py
model_type: UIP # see `ModelType` enum in matbench_discovery/enums.py
model_params: 5_000_000
trained_for_benchmark: true
n_estimators: 1

license:
  code: MIT
  code_url: https://github.com/mir-group/nequip/blob/main/LICENSE
  checkpoint: CC-BY-4.0
  # URL that points to the license file for the model checkpoint, not the checkpoint file itself.
  checkpoint_url: https://url.of/model-checkpoint-license # TODO: Zenodo? / Nequip.net

hyperparams: # strongly recommended to list relaxation hyperparams
  max_force: 0.05
  max_steps: 500
  ase_optimizer: GOQN # faster than FIRE with same results; see SI of https:/doi.org/10.1088/2515-7655/ade916
  cell_filter: FrechetCellFilter
  optimizer: AdamW
  weight_decay: 1e-3
  graph_construction_radius: 6.0
  sph_harmonics_l_max: 3
  n_layers: 5
  n_features: 96
  parity: false
  zbl_potential: false
  allegro_mlp_depth: 3
  allegro_mlp_width: 1024
  tensor_path_channel_coupling: true
  polynomial_cutoff: 8
  n_radial_bessel_basis: 12
  loss: Huber - delta=0.01 for energy, delta=0.1 for stress, stratified delta (0.01, 0.007, 0.004, 0.001) for force
  loss_weights:
    energy: 1.0
    force: 5.0
    stress: 0.01
  batch_size: 520 # 8 (gpus) * 65 (batch per gpu) = 520 (total batch size)
  initial_learning_rate: 0.01
  gradient_clip_val: 0.015
  learning_rate_schedule: ReduceLROnPlateau - factor=0.5, patience=250, min_lr=1e-6
  epochs: 250
  max_neighbors: .inf

training_cost: # list any hardware used to train the model and for how long
  Nvidia H100 GPUs: { amount: 8, hours: 50, cost: 400 }

requirements: # strongly recommended
  torch: '2'
  nequip: 0.7.0 # >=0.14.0 recommended
  allegro: 0.5.0 # >=0.7.1 recommended

training_set: [MPtrj]

notes: # notes can have any key, be multiline and support markdown.
  description: Large  'compliant' Allegro foundation potential; see <https://www.nequip.net/models/mir-group/Allegro-MP-L:0.1> for details and <https://arxiv.org/abs/2504.16068> for model/training infrastructure.
  steps: |
    Single training run on MPtrj with specified parameters.

metrics:
  phonons:
    kappa_103:
      pred_file: models/allegro/Allegro-MP-L-0.1/Allegro-MP-L-0.1-2025-08-27-kappa-103-FIRE-dist_0_03-fmax_0_0001-symprec_1e-05.json.gz
      pred_file_url: https://figshare.com/files/57576301
      κ_SRME: 0.5039
      κ_SRE: 0.2309
  geo_opt:
    pred_file: models/allegro/Allegro-MP-L-0.1/Allegro-MP-L-0.1-2025-08-27-wbm-IS2RE-GOQN.jsonl.gz
    pred_file_url: https://figshare.com/files/57731014
    struct_col: nequip_structure # same for NequIP/Allegro
    symprec=1e-5:
      rmsd: 0.0816 # unitless
      n_sym_ops_mae: 1.902 # unitless
      symmetry_decrease: 0.0712 # fraction
      symmetry_match: 0.7211 # fraction
      symmetry_increase: 0.2015 # fraction
      n_structures: 256963 # count
      analysis_file: models/allegro/Allegro-MP-L-0.1/Allegro-MP-L-0.1-2025-08-27-wbm-IS2RE-GOQN-symprec=1e-5-moyo=0.4.4.csv.gz
      analysis_file_url: https://figshare.com/files/57731050
    symprec=1e-2:
      rmsd: 0.0816 # unitless
      n_sym_ops_mae: 1.7918 # unitless
      symmetry_decrease: 0.0572 # fraction
      symmetry_match: 0.8125 # fraction
      symmetry_increase: 0.123 # fraction
      n_structures: 256963 # count
      analysis_file: models/allegro/Allegro-MP-L-0.1/Allegro-MP-L-0.1-2025-08-27-wbm-IS2RE-GOQN-symprec=1e-2-moyo=0.4.4.csv.gz
      analysis_file_url: https://figshare.com/files/57731053
  discovery:
    pred_file: models/allegro/Allegro-MP-L-0.1/Allegro-MP-L-0.1-2025-08-27-wbm-IS2RE.csv.gz
    pred_file_url: https://figshare.com/files/57575095
    pred_col: e_form_per_atom_nequip # same for NequIP/Allegro
    full_test_set:
      F1: 0.742 # fraction
      DAF: 3.998 # dimensionless
      Precision: 0.686 # fraction
      Recall: 0.809 # fraction
      Accuracy: 0.904 # fraction
      TPR: 0.809 # fraction
      FPR: 0.077 # fraction
      TNR: 0.923 # fraction
      FNR: 0.191 # fraction
      TP: 35671.0 # count
      FP: 16330.0 # count
      TN: 196541.0 # count
      FN: 8421.0 # count
      MAE: 0.042 # eV/atom
      RMSE: 0.085 # eV/atom
      R2: 0.775 # dimensionless
      missing_preds: 2 # count
    unique_prototypes:
      F1: 0.751 # fraction
      DAF: 4.516 # dimensionless
      Precision: 0.69 # fraction
      Recall: 0.823 # fraction
      Accuracy: 0.915 # fraction
      TPR: 0.823 # fraction
      FPR: 0.068 # fraction
      TNR: 0.932 # fraction
      FNR: 0.177 # fraction
      TP: 27483.0 # count
      FP: 12329.0 # count
      TN: 169785.0 # count
      FN: 5891.0 # count
      MAE: 0.044 # eV/atom
      RMSE: 0.087 # eV/atom
      R2: 0.778 # dimensionless
      missing_preds: 0 # count
    most_stable_10k:
      F1: 0.917 # fraction
      DAF: 5.536 # dimensionless
      Precision: 0.846 # fraction
      Recall: 1.0 # fraction
      Accuracy: 0.846 # fraction
      TPR: 1.0 # fraction
      FPR: 1.0 # fraction
      TNR: 0.0 # fraction
      FNR: 0.0 # fraction
      TP: 8463.0 # count
      FP: 1537.0 # count
      TN: 0.0 # count
      FN: 0.0 # count
      MAE: 0.064 # eV/atom
      RMSE: 0.124 # eV/atom
      R2: 0.706 # dimensionless
      missing_preds: 0 # count
