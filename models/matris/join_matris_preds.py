"""Concatenate MatRIS results from multiple data files generated by slurm job array into
single file."""
# /// script
# requires-python = ">=3.10,<3.14"
# dependencies = [
# "torch==2.6.0",
# "numpy==2.3.4",
# "ase==3.26.0",
# "pymatgen==2025.10.7",
# "pymatviz==0.17.2",
# "scikit-learn==1.7.2",
# "tqdm==4.67.1",
# "matbench-discovery"==1.3.1,
# ]
#
# [tool.uv.sources]
# matbench-discovery = { path = "../../", editable = true }
# ///
# %%
import os
from glob import glob

import pandas as pd
import pymatviz as pmv
from pymatgen.core import Structure
from pymatgen.entries.compatibility import MaterialsProject2020Compatibility
from pymatgen.entries.computed_entries import ComputedStructureEntry
from pymatviz.enums import Key
from tqdm import tqdm

from matbench_discovery.data import DataFiles, as_dict_handler
from matbench_discovery.energy import get_e_form_per_atom
from matbench_discovery.enums import MbdKey, Model, Task
from matbench_discovery.preds.discovery import df_preds

module_dir = os.path.dirname(__file__)


# %%
task_type = Task.IS2RE
date = "2025-10-29"
model_key = Model.matris_10m_mp.key

glob_pattern = f"{model_key}/{date}-wbm-{task_type}*/*.json.gz"
file_paths = sorted(glob(f"{module_dir}/{glob_pattern}"))
print(f"Found {len(file_paths):,} files for {glob_pattern = }")
dfs: dict[str, pd.DataFrame] = {}


# %%
failed = {}
for file_path in tqdm(file_paths):
    if file_path in dfs:
        continue
    try:
        # df_i = pd.read_json(file_path).set_index(Key.mat_id)
        df_i = pd.read_json(file_path, lines=True).set_index(Key.mat_id)
    except Exception as exc:
        failed[file_path] = str(exc)
        continue
    # drop trajectory to save memory
    dfs[file_path] = df_i.drop(columns="matris_trajectory", errors="ignore")

err_counts = pd.Series(failed).value_counts()
print(f"{err_counts=}")

df_matris = pd.concat(dfs.values()).round(4)

df_wbm_cse = pd.read_json(
    DataFiles.wbm_computed_structure_entries.path, lines=True
).set_index(Key.mat_id)

# Key.cse occurs error (pymatviz==0.17.2):
# type object 'Key' has no attribute 'cse'.
CSE = "computed_structure_entry"
df_wbm_cse[CSE] = [
    ComputedStructureEntry.from_dict(dct) for dct in tqdm(df_wbm_cse[CSE])
]
cse: ComputedStructureEntry
e_col = "matris_energy"
struct_col = "matris_structure"
e_form_matris_col = "e_form_per_atom_matris"

for mat_id in tqdm(df_matris.index):
    matris_energy = df_matris.loc[mat_id, e_col]
    mlip_struct = Structure.from_dict(df_matris.loc[mat_id, struct_col])
    cse = df_wbm_cse.loc[mat_id, CSE]
    cse._energy = matris_energy  # cse._energy is the uncorrected energy  # noqa: SLF001
    cse._structure = mlip_struct  # noqa: SLF001
    df_matris.loc[mat_id, CSE] = cse

# MP2020
processed = MaterialsProject2020Compatibility().process_entries(
    df_matris[CSE], verbose=True, clean=True
)
if len(processed) != len(df_matris):
    raise ValueError(f"not all entries processed: {len(processed)=} {len(df_matris)=}")

df_matris[e_form_matris_col] = [
    get_e_form_per_atom(cse) for cse in tqdm(df_matris[CSE])
]

df_preds[e_form_matris_col] = df_matris[e_form_matris_col]

out_path = file_paths[0].rsplit("/", 1)[0]
df_matris = df_matris.round(4)
df_matris.select_dtypes("number").to_csv(f"{out_path}.csv.gz")
df_matris.reset_index().to_json(
    f"{out_path}.json.gz", default_handler=as_dict_handler, orient="records", lines=True
)
fig = pmv.density_scatter(df=df_preds, x=MbdKey.e_form_dft, y=e_form_matris_col)
img_path = f"{module_dir}/{model_key}-e-form-parity.html"
fig.show()
pmv.save_fig(fig, img_path)
