model_name: eqV2 M
model_version: v2024.10.18
matbench_discovery_version: 1.3.1
date_added: "2024-10-18"
date_published: "2024-10-18"
authors:
  - name: Luis Barroso-Luque
    affiliation: FAIR Meta
    email: lbluque@meta.com
    orcid: https://orcid.org/0000-0002-6453-9545
    github: https://github.com/lbluque
    corresponding: true
  - name: Muhammed Shuaibi
    affiliation: FAIR Meta
  - name: Xiang Fu
    affiliation: FAIR Meta
  - name: Brandon M. Wood
    affiliation: FAIR Meta
  - name: Misko Dzamba
    affiliation: FAIR Meta
  - name: Meng Gao
    affiliation: FAIR Meta
  - name: Ammar Rizvi
    affiliation: FAIR Meta
  - name: C.~Lawrence Zitnick
    affiliation: FAIR Meta
  - name: Zachary W. Ulissi
    affiliation: FAIR Meta
    email: zulissi@meta.com
    orcid: https://orcid.org/0000-0002-9401-4918
    corresponding: true

repo: https://github.com/FAIR-Chem/fairchem
doi: https://doi.org/10.48550/arXiv.2410.12771
paper: https://arxiv.org/abs/2410.12771
url: https://huggingface.co/fairchem/OMAT24
pypi: https://pypi.org/project/fairchem-core

requirements:
  fairchem-core: 1.2.1

openness: OSOD
trained_for_benchmark: true
train_task: S2EFS
test_task: IS2RE-SR
targets: EFS_D
model_type: UIP
model_params: 86_589_068
n_estimators: 1

# removed sAlex from this list since it would be double counting materials in the "Training Size"
# metrics table column since OMat24 is a derivative of Alexandria
training_set: [OMat24, MPtrj]

hyperparams:
  max_force: 0.02
  max_steps: 500
  ase_optimizer: FIRE
  loss: MAE
  loss_weights:
    energy: 20
    forces: 10
    stress: 1
  optimizer: AdamW
  learning_rate_schedule: Cosine
  warmup_epochs: 0.1
  warmup_factor: 0.2
  max_learning_rate: 0.0002
  min_learning_rate_factor: 0.01
  grad_clip_threshold: 100
  ema_decay: 0.999
  weight_decay: 0.001
  dropout_rate: 0.1
  stochastic_depth: 0.1
  batch_size: 256
  epochs: 16
  # train_config_url: #TODO add URL

notes:
  Description: |
    EquiformerV2 is an equivariant transformer that uses graph attention, attention re-normalization, and separable S^2 activations and layer normalization.
  Training: |
    Training was done by fine-tuning a model pretrained for 2 epochs on the OMat24 dataset.

metrics:
  geo_opt:
    pred_file: null # authors will share model-relaxed structures soon
    pred_col: null
  discovery:
    pred_file: models/eqV2/eqV2-m-omat-mp-salex.csv.gz
    pred_col: e_form_per_atom_eqV2-86M-omat-mp-salex
    full_test_set:
      F1: 0.896
      DAF: 5.243
      Precision: 0.9
      Recall: 0.893
      Accuracy: 0.965
      TPR: 0.893
      FPR: 0.021
      TNR: 0.979
      FNR: 0.107
      TP: 39379.0
      FP: 4393.0
      TN: 208478.0
      FN: 4713.0
      MAE: 0.02
      RMSE: 0.071
      R2: 0.842
      missing_preds: 2
      missing_percent: 0.00%
    most_stable_10k:
      F1: 0.988
      DAF: 6.382
      Precision: 0.976
      Recall: 1.0
      Accuracy: 0.976
      TPR: 1.0
      FPR: 1.0
      TNR: 0.0
      FNR: 0.0
      TP: 9756.0
      FP: 244.0
      TN: 0.0
      FN: 0.0
      MAE: 0.015
      RMSE: 0.066
      R2: 0.904
      missing_preds: 0
      missing_percent: 0.00%
    unique_prototypes:
      F1: 0.917
      DAF: 6.047
      Precision: 0.924
      Recall: 0.91
      Accuracy: 0.975
      TPR: 0.91
      FPR: 0.014
      TNR: 0.986
      FNR: 0.09
      TP: 30372.0
      FP: 2481.0
      TN: 179633.0
      FN: 3002.0
      MAE: 0.02
      RMSE: 0.072
      R2: 0.848
      missing_preds: 0
      missing_percent: 0.00%
