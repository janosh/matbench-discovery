model_name: eqV2 M
model_key: eqV2-m-omat-mp-salex
model_version: v2024.10.18
matbench_discovery_version: 1.3.1
date_added: "2024-10-18"
date_published: "2024-10-18"
authors:
  - name: Luis Barroso-Luque
    affiliation: FAIR Meta
    email: lbluque@meta.com
    orcid: https://orcid.org/0000-0002-6453-9545
    github: https://github.com/lbluque
    corresponding: true
  - name: Muhammed Shuaibi
    affiliation: FAIR Meta
  - name: Xiang Fu
    affiliation: FAIR Meta
  - name: Brandon M. Wood
    affiliation: FAIR Meta
  - name: Misko Dzamba
    affiliation: FAIR Meta
  - name: Meng Gao
    affiliation: FAIR Meta
  - name: Ammar Rizvi
    affiliation: FAIR Meta
  - name: C.~Lawrence Zitnick
    affiliation: FAIR Meta
  - name: Zachary W. Ulissi
    affiliation: FAIR Meta
    email: zulissi@meta.com
    orcid: https://orcid.org/0000-0002-9401-4918
    corresponding: true

repo: https://github.com/FAIR-Chem/fairchem
doi: https://doi.org/10.48550/arXiv.2410.12771
paper: https://arxiv.org/abs/2410.12771
url: https://huggingface.co/fairchem/OMAT24
pypi: https://pypi.org/project/fairchem-core
pr_url: https://github.com/janosh/matbench-discovery/pull/146

requirements:
  fairchem-core: 1.2.1

openness: OSOD
trained_for_benchmark: true
train_task: S2EFS
test_task: IS2RE-SR
targets: EFS_D
model_type: UIP
model_params: 86_589_068
n_estimators: 1

# removed sAlex from this list since it would be double counting materials in the "Training Size"
# metrics table column since OMat24 is a derivative of Alexandria
training_set: [OMat24, MPtrj]

hyperparams:
  max_force: 0.02
  max_steps: 500
  ase_optimizer: FIRE
  loss: MAE
  loss_weights:
    energy: 20
    forces: 10
    stress: 1
  optimizer: AdamW
  learning_rate_schedule: Cosine
  warmup_epochs: 0.1
  warmup_factor: 0.2
  max_learning_rate: 0.0002
  min_learning_rate_factor: 0.01
  grad_clip_threshold: 100
  ema_decay: 0.999
  weight_decay: 0.001
  dropout_rate: 0.1
  stochastic_depth: 0.1
  batch_size: 256
  epochs: 16

notes:
  Description: |
    EquiformerV2 is an equivariant transformer that uses graph attention, attention re-normalization, and separable S^2 activations and layer normalization.
  Training: |
    Training was done by fine-tuning a model pretrained for 2 epochs on the OMat24 dataset.

metrics:
  phonons:
    kappa_103:
      κ_SRME: 1.771
      pred_file: models/eqV2/eqV2-m-omat-mp-salex/2024-11-09-kappa-103-FIRE-dist=0.01-fmax=1e-4-symprec=1e-5.json.gz
      pred_file_url: https://figshare.com/ndownloader/files/52134893
  geo_opt:
    pred_file: models/eqV2/eqV2-m-omat-mp-salex/2024-10-18-wbm-geo-opt.json.gz
    pred_file_url: https://figshare.com/ndownloader/files/51607436
    struct_col: eqV2-86M-omat-mp-salex_structure
    symprec=1e-5:
      rmsd: 0.0138 # Å
      n_sym_ops_mae: 10.0558 # unitless
      symmetry_decrease: 0.8611 # fraction
      symmetry_match: 0.1382 # fraction
      symmetry_increase: 0.0005 # fraction
      n_structures: 256614 # count
      analysis_file: models/eqV2/eqV2-m-omat-mp-salex/2024-10-18-wbm-geo-opt-symprec=1e-5.csv.gz
      analysis_file_url: https://figshare.com/ndownloader/files/52059485
    symprec=1e-2:
      rmsd: 0.0112 # Å
      n_sym_ops_mae: 2.077 # unitless
      symmetry_decrease: 0.1321 # fraction
      symmetry_match: 0.7474 # fraction
      symmetry_increase: 0.1077 # fraction
      n_structures: 256614 # count
      analysis_file: models/eqV2/eqV2-m-omat-mp-salex/2024-10-18-wbm-geo-opt-symprec=1e-2.csv.gz
      analysis_file_url: https://figshare.com/ndownloader/files/52059488
  discovery:
    pred_file: models/eqV2/eqV2-m-omat-salex-mp/2024-10-18-wbm-IS2RE.csv.gz
    pred_file_url: https://figshare.com/ndownloader/files/52057571
    pred_col: e_form_per_atom_eqV2-86M-omat-mp-salex
    full_test_set:
      F1: 0.896 # fraction
      DAF: 5.243 # dimensionless
      Precision: 0.9 # fraction
      Recall: 0.893 # fraction
      Accuracy: 0.965 # fraction
      TPR: 0.893 # fraction
      FPR: 0.021 # fraction
      TNR: 0.979 # fraction
      FNR: 0.107 # fraction
      TP: 39379.0 # count
      FP: 4393.0 # count
      TN: 208478.0 # count
      FN: 4713.0 # count
      MAE: 0.02 # eV/atom
      RMSE: 0.071 # eV/atom
      R2: 0.842 # dimensionless
      missing_preds: 2 # count
      missing_percent: 0.00% # fraction
    most_stable_10k:
      F1: 0.988 # fraction
      DAF: 6.382 # dimensionless
      Precision: 0.976 # fraction
      Recall: 1.0 # fraction
      Accuracy: 0.976 # fraction
      TPR: 1.0 # fraction
      FPR: 1.0 # fraction
      TNR: 0.0 # fraction
      FNR: 0.0 # fraction
      TP: 9756.0 # count
      FP: 244.0 # count
      TN: 0.0 # count
      FN: 0.0 # count
      MAE: 0.015 # eV/atom
      RMSE: 0.066 # eV/atom
      R2: 0.904 # dimensionless
      missing_preds: 0 # count
      missing_percent: 0.00% # fraction
    unique_prototypes:
      F1: 0.917 # fraction
      DAF: 6.047 # dimensionless
      Precision: 0.924 # fraction
      Recall: 0.91 # fraction
      Accuracy: 0.975 # fraction
      TPR: 0.91 # fraction
      FPR: 0.014 # fraction
      TNR: 0.986 # fraction
      FNR: 0.09 # fraction
      TP: 30372.0 # count
      FP: 2481.0 # count
      TN: 179633.0 # count
      FN: 3002.0 # count
      MAE: 0.02 # eV/atom
      RMSE: 0.072 # eV/atom
      R2: 0.848 # dimensionless
      missing_preds: 0 # count
      missing_percent: 0.00% # fraction
